# Documentation Vectorisation GraphRAG v3.0

**Version** : 3.0  
**Date** : 2025-10-11  
**Statut** : ‚úÖ Production - RAG Hybride avec Gemini Embeddings

---

## üìã TABLE DES MATI√àRES

1. [Vue d'Ensemble](https://claude.ai/chat/b2c13759-b3c9-4705-97d6-e088f58b4b77#vue-densemble)
2. [Configuration GCP et Vertex AI](https://claude.ai/chat/b2c13759-b3c9-4705-97d6-e088f58b4b77#configuration-gcp-et-vertex-ai)
3. [Architecture RAG Hybride](https://claude.ai/chat/b2c13759-b3c9-4705-97d6-e088f58b4b77#architecture-rag-hybride)
4. [Pipeline Documents](https://claude.ai/chat/b2c13759-b3c9-4705-97d6-e088f58b4b77#pipeline-documents)
5. [Pipeline Entit√©s](https://claude.ai/chat/b2c13759-b3c9-4705-97d6-e088f58b4b77#pipeline-entit%C3%A9s)
6. [Index Vectoriel](https://claude.ai/chat/b2c13759-b3c9-4705-97d6-e088f58b4b77#index-vectoriel)
7. [Utilisation](https://claude.ai/chat/b2c13759-b3c9-4705-97d6-e088f58b4b77#utilisation)
8. [Validation](https://claude.ai/chat/b2c13759-b3c9-4705-97d6-e088f58b4b77#validation)
9. [Troubleshooting](https://claude.ai/chat/b2c13759-b3c9-4705-97d6-e088f58b4b77#troubleshooting)

---

## üéØ Vue d'Ensemble

### Objectif

Syst√®me RAG (Retrieval-Augmented Generation) hybride combinant :

- **Recherche vectorielle s√©mantique** (embeddings 768D via Gemini)
- **Recherche structur√©e** (graphe Neo4j)
- **Compatible Claude Projects** (Vertex AI native)

### Configuration

```json
{
  "embedding_model": "text-multilingual-embedding-002",
  "provider": "Vertex AI",
  "dimensions": 768,
  "similarity": "cosine",
  "gcp": {
    "project_id": "gen-lang-client-0810997704",
    "location": "us-central1"
  }
}
```

### Nouveaut√©s v3.0

- ‚úÖ **Gemini Embeddings** natifs (multilingue FR/DE)
- ‚úÖ **Rate limiting** intelligent (√©vite erreurs 429)
- ‚úÖ **Exponential backoff** automatique
- ‚úÖ **Compatible Claude Projects** sans serveur interm√©diaire
- ‚úÖ **Fuzzy matching** pour corpus OCR imparfait
- ‚úÖ **Contexte adaptatif** selon longueur citation

### Architecture en 2 Pipelines

```
PIPELINE 1 : DOCUMENTS (quote_centered)
sources_md/*.md ‚Üí Chunks textuels ‚Üí Gemini Embeddings ‚Üí Neo4j
‚Üì
Assertions ‚Üí Events/MicroActions
‚Üì
Relations: DESCRIBES_EVENT, DESCRIBES_ACTION, MENTIONS

PIPELINE 2 : ENTIT√âS (entity_summary)
Person/Organization/GPE ‚Üí Notices + Structures ‚Üí Gemini Embeddings ‚Üí Neo4j
‚Üì
Relations: DESCRIBES_ENTITY, MENTIONS
```

---

## üîê Configuration GCP et Vertex AI

### Pr√©requis

1. **Compte Google Cloud Platform**
    
    - Cr√©dits gratuits : 300$ pendant 90 jours
    - Inscription : https://cloud.google.com/
2. **Projet GCP cr√©√©**
    
    - Project ID : `gen-lang-client-0810997704` (exemple)
    - Location : `us-central1` (recommand√©)
3. **API Vertex AI activ√©e**
    
    - Console : https://console.cloud.google.com/apis/library
    - Chercher "Vertex AI API" et activer

### Authentification

#### Option A : Service Account (Recommand√© pour production)

```bash
# 1. Cr√©er service account dans GCP Console
# IAM & Admin ‚Üí Service Accounts ‚Üí CREATE SERVICE ACCOUNT
# Nom : vertex-ai-embeddings
# Role : Vertex AI User

# 2. Cr√©er cl√© JSON
# KEYS ‚Üí ADD KEY ‚Üí Create new key ‚Üí JSON

# 3. Placer le fichier
mkdir -p ~/.gcp
mv ~/Downloads/gen-lang-client-*.json ~/.gcp/vertex-ai-key.json
chmod 600 ~/.gcp/vertex-ai-key.json

# 4. Configurer variable d'environnement
export GOOGLE_APPLICATION_CREDENTIALS="$HOME/.gcp/vertex-ai-key.json"

# 5. Rendre permanent
echo 'export GOOGLE_APPLICATION_CREDENTIALS="$HOME/.gcp/vertex-ai-key.json"' >> ~/.bashrc
source ~/.bashrc
```

#### Option B : gcloud CLI (Alternative)

```bash
# Installation gcloud CLI
# Voir : https://cloud.google.com/sdk/docs/install

# Authentification
gcloud auth application-default login
gcloud config set project gen-lang-client-0810997704
```

### Installation D√©pendances Python

```bash
pip install google-cloud-aiplatform neo4j rapidfuzz
```

### Test de Configuration

```python
# test_vertex_ai.py
from google.cloud import aiplatform
from vertexai.language_models import TextEmbeddingModel

aiplatform.init(project="gen-lang-client-0810997704", location="us-central1")
model = TextEmbeddingModel.from_pretrained("text-multilingual-embedding-002")

text = "Elisabeth M√ºller √©tait d√©tenue au camp de Gurs."
embeddings = model.get_embeddings([text])
print(f"‚úÖ Embedding g√©n√©r√© : {len(embeddings[0].values)} dimensions")
```

### Quotas et Limites

**Quotas par d√©faut (nouveaux comptes)** :

- 60 requ√™tes/minute pour embeddings
- Peut causer erreurs `429 Quota exceeded`

**Solutions** :

1. **Rate limiting dans le code** (impl√©ment√© dans v3.0)
2. **Augmenter quota** via https://console.cloud.google.com/iam-admin/quotas
    - Filtre : `aiplatform.googleapis.com/online_prediction_requests_per_base_model`
    - Demander : 600 requ√™tes/minute
    - D√©lai approbation : 24-48h

**Co√ªts** :

- text-multilingual-embedding-002 : ~0.00001$ par 1000 caract√®res
- Pour 400 chunks √ó 800 chars = **0.0032$** (moins d'un centime)

---

## üóÇÔ∏è Architecture RAG Hybride

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              CORPUS DIPLOMATIQUE (Neo4j Aura)            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ  DOCUMENTS (75)  ‚îÇ         ‚îÇ  ENTIT√âS (100)   ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  ArchiveDocument ‚îÇ         ‚îÇ  Person (48)     ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îÇ  Org (29)        ‚îÇ      ‚îÇ
‚îÇ           ‚îÇ                   ‚îÇ  GPE (23)        ‚îÇ      ‚îÇ
‚îÇ           ‚îÇ                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ           ‚ñº                            ‚ñº                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
‚îÇ  ‚îÇ CHUNKS DOCUMENTS ‚îÇ         ‚îÇ CHUNKS ENTIT√âS   ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ  quote_centered  ‚îÇ         ‚îÇ  entity_summary  ‚îÇ      ‚îÇ
‚îÇ  ‚îÇ     (~225)       ‚îÇ         ‚îÇ     (91)         ‚îÇ      ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
‚îÇ           ‚îÇ                            ‚îÇ                ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ                        ‚ñº                                ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                       ‚îÇ
‚îÇ              ‚îÇ  INDEX VECTORIEL ‚îÇ                       ‚îÇ
‚îÇ              ‚îÇ  chunk_embeddings‚îÇ                       ‚îÇ
‚îÇ              ‚îÇ      (~316)      ‚îÇ                       ‚îÇ
‚îÇ              ‚îÇ  768D - cosine   ‚îÇ                       ‚îÇ
‚îÇ              ‚îÇ  Gemini Native   ‚îÇ                       ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                       ‚îÇ
‚îÇ                       ‚îÇ                                 ‚îÇ
‚îÇ           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                     ‚îÇ
‚îÇ           ‚ñº                       ‚ñº                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇ STRUCTURES      ‚îÇ    ‚îÇ  √âV√âNEMENTS     ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ Events (92)     ‚îÇ    ‚îÇ  MicroActions   ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ MicroActions    ‚îÇ    ‚îÇ  (202)          ‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

REQU√äTE UTILISATEUR (Claude Projects)
        ‚îÇ
        ‚ñº
  [Vertex AI Embedding]
  text-multilingual-embedding-002
        ‚îÇ
        ‚ñº
  [Recherche Vectorielle Neo4j]
        ‚îÇ
        ‚ñº
  [Chunks similaires 768D cosine]
        ‚îÇ
        ‚îú‚îÄ> Documents ‚Üí Events/Actions ‚Üí Context
        ‚îî‚îÄ> Entit√©s ‚Üí Profils ‚Üí Context
                ‚îÇ
                ‚ñº
        [R√©ponse enrichie Claude]
```

### Total RAG Hybride (Production)

```
TOTAL CHUNKS VECTORIS√âS : ~316
  ‚îú‚îÄ Documents (quote_centered) : ~225
  ‚îî‚îÄ Entit√©s (entity_summary) : 91 (9 √©checs mineurs)

TOTAL EMBEDDINGS : ~316
  ‚îî‚îÄ Dimension uniforme : 768D
  ‚îî‚îÄ Provider : Vertex AI
  ‚îî‚îÄ Model : text-multilingual-embedding-002

TOTAL RELATIONS :
  ‚îú‚îÄ CHUNK_OF : ~225 (documents ‚Üí archives)
  ‚îú‚îÄ DESCRIBES_EVENT : ~75
  ‚îú‚îÄ DESCRIBES_ACTION : ~150
  ‚îú‚îÄ DESCRIBES_ENTITY : 91
  ‚îî‚îÄ MENTIONS : ~1102 (documents + entit√©s)

INDEX VECTORIEL : chunk_embeddings
  ‚îú‚îÄ Dimension : 768
  ‚îú‚îÄ Similarit√© : cosine
  ‚îú‚îÄ Provider : Neo4j Vector Index
  ‚îî‚îÄ Compatible : Vertex AI native
```

---

## üìÑ Pipeline Documents (quote_centered)

### Objectif

Vectoriser les documents sources pour recherche s√©mantique dans les archives avec strat√©gie **Quote-First**.

### Strat√©gie Quote-First

Au lieu de d√©couper arbitrairement les documents en chunks fixes, on centre les chunks sur les **citations sources des Assertions**.

**Avantages** :

- ‚úÖ Pr√©cision maximale : lien direct chunk ‚Üí √©v√©nement
- ‚úÖ Pas de faux positifs
- ‚úÖ Relations tra√ßables et explicables
- ‚úÖ Coverage √©lev√© (85% typique pour corpus OCR)

### √âtape 1 : Localisation des Quotes

```python
def locate_quote_in_document(quote: str, document: str):
    """
    Localise quote dans document avec fuzzy matching
    """
    # 1. Normalisation
    quote_norm = normalize(quote)      # Enlever guillemets, espaces...
    doc_norm = normalize(document)     # Idem
    
    # 2. Exact match
    pos = doc_norm.find(quote_norm)
    if pos != -1:
        return pos, "exact"
    
    # 3. Fuzzy match (fallback pour OCR imparfait)
    if use_fuzzy:
        pos = fuzzy_search(quote_norm, doc_norm, threshold=85%)
        if pos != -1:
            return pos, "fuzzy"
    
    return -1, None
```

**R√©sultats typiques** :

- Exact matches : 22%
- Fuzzy matches : 78%
- Total localis√© : 85-90%

### √âtape 2 : Chunking Adaptatif

```python
def create_quote_centered_chunk(quote_pos, quote_length, document):
    """
    Cr√©e chunk avec contexte adaptatif
    """
    # Contexte adaptatif selon longueur quote
    if quote_length < 100:
        context_size = 200
    elif quote_length < 300:
        context_size = 350
    else:
        context_size = 450
    
    # Bornes totales (400-900 chars)
    total = quote_length + (2 * context_size)
    if total < 400:
        context_size = (400 - quote_length) // 2
    elif total > 900:
        context_size = (900 - quote_length) // 2
    
    # Extraction
    start = max(0, quote_pos - context_size)
    end = min(len(document), quote_pos + quote_length + context_size)
    
    # Extension aux fronti√®res paragraphe (si proche < 50 chars)
    start, end = extend_to_paragraph_boundaries(start, end, document)
    
    return document[start:end]
```

### √âtape 3 : G√©n√©ration Embeddings via Gemini

```python
from google.cloud import aiplatform
from vertexai.language_models import TextEmbeddingModel
import time

def get_embedding_gemini(text: str):
    """
    G√©n√®re embedding via Gemini avec rate limiting
    """
    max_retries = 5
    base_delay = 1.0  # Rate limiting : 1s entre requ√™tes
    
    for attempt in range(max_retries):
        try:
            time.sleep(base_delay)  # √âviter quota 429
            embeddings = model.get_embeddings([text])
            return embeddings[0].values  # 768D
            
        except Exception as e:
            if "429" in str(e) or "Quota exceeded" in str(e):
                wait_time = base_delay * (2 ** attempt)  # Exponential backoff
                print(f"‚è≥ Quota exceeded, waiting {wait_time:.1f}s...")
                time.sleep(wait_time)
                continue
            elif attempt < max_retries - 1:
                time.sleep(2)
                continue
            else:
                return []
    
    return []
```

### √âtape 4 : Cr√©ation N≈ìuds Neo4j

```cypher
CREATE (c:Chunk {
  id: "{doc_id}::chunk_{idx}",
  text: "... contexte + citation ...",
  chunk_type: "quote_centered",
  embedding: [...],  // 768D Gemini
  embedding_model: "text-multilingual-embedding-002",
  embedding_provider: "vertex-ai",
  start_char: 1234,
  end_char: 2834,
  doc_id: "/id/document/{sha1}",
  year: 1942,
  assertion_id: "/id/assertion/{sha1}",
  match_method: "fuzzy"  // ou "exact"
})
```

### √âtape 5 : Cr√©ation Relations

```cypher
// Lien au document
MATCH (c:Chunk {id: $chunk_id})
MATCH (d:ArchiveDocument {id: $doc_id})
MERGE (c)-[:CHUNK_OF]->(d)

// Lien √©v√©nements
MATCH (c:Chunk {id: $chunk_id})
MATCH (e:Event {event_id: $event_id})
MERGE (c)-[r:DESCRIBES_EVENT]->(e)
SET r.method = $match_method,
    r.confidence = CASE WHEN $match_method = 'exact' THEN 1.0 ELSE 0.9 END,
    r.via_assertion = $assertion_id

// Lien micro-actions
MATCH (c:Chunk {id: $chunk_id})
MATCH (m:MicroAction {micro_id: $micro_id})
MERGE (c)-[r:DESCRIBES_ACTION]->(m)
SET r.method = $match_method,
    r.confidence = CASE WHEN $match_method = 'exact' THEN 1.0 ELSE 0.9 END

// Mentions entit√©s (extraction depuis IDs dans texte)
MATCH (c:Chunk)
WHERE c.text CONTAINS "/id/person/1c6bf450"
MATCH (p:Person {id: "/id/person/1c6bf450-3c41-4755-be82-d77f9744f6e1"})
MERGE (c)-[:MENTIONS]->(p)
```

---

## üë§ Pipeline Entit√©s (entity_summary)

### Objectif

Cr√©er profils vectoris√©s pour recherche biographique/institutionnelle enrichie.

### √âtape 1 : Assemblage Texte Multi-Sources

#### Person (avec structures r√©ifi√©es)

```python
def assemble_person_text(person_id):
    parts = []
    
    # === PARTIE 1 : NEO4J ===
    # Identit√©
    parts.append(f"{prefLabel_fr}")
    if prefLabel_de and prefLabel_de != prefLabel_fr:
        parts.append(f"({prefLabel_de})")
    if aliases:
        parts.append(f"Alias: {', '.join(aliases[:3])}")
    
    # Notice biographique (principale)
    if notice_bio:
        parts.append(f"\n\n{notice_bio}")
    
    # Sources archivistiques
    sources = get_archive_sources(person_id)
    if sources:
        parts.append(f"\n\nSources: {', '.join(sources[:3])}")
    
    # === PARTIE 2 : MARKDOWN ===
    # Sections narratives enrichies
    sections = extract_narrative_sections(person_id)
    if sections.get('lieux_residence'):
        parts.append(f"\n\nLieux de r√©sidence: {sections['lieux_residence']}")
    if sections.get('notes_recherche'):
        parts.append(f"\n\nNotes de recherche: {sections['notes_recherche']}")
    if sections.get('contexte_relationnel'):
        parts.append(f"\n\nContexte relationnel: {sections['contexte_relationnel']}")
    
    # === PARTIE 3 : STRUCTURES R√âIFI√âES ===
    # Occupations (avec IDs pour MENTIONS)
    occupations = get_occupations(person_id)
    if occupations:
        # Format: "position_title √† org_name /id/org/xxx (interval)"
        parts.append(f"\n\nOccupations: {occupations}")
    
    # Origins
    origins = get_origins(person_id)
    if origins:
        # Format: "mode place_name /id/gpe/xxx (interval)"
        parts.append(f"\n\nOrigines: {origins}")
    
    # Family Relations
    family_rels = get_family_relations(person_id)
    if family_rels:
        # Format: "relation_type de target_name /id/person/xxx (interval)"
        parts.append(f"\n\nRelations familiales: {family_rels}")
    
    return " ".join(parts)
```

**Exemple de texte assembl√©** :

```
Elisabeth M√ºller (M√ºller Elisabeth). Alias: Elise, Betty. 

N√©e en 1896 √† B√¢le. R√©fugi√©e juive allemande intern√©e en France pendant la Seconde Guerre mondiale.

Sources: E2001E#1000/1571#220*, E2001E#1000/1571#221*

Lieux de r√©sidence: B√¢le /id/gpe/xxx (1896-1933), Paris /id/gpe/yyy (1933-1940)

Occupations: Employ√©e domestique √† Organisation X /id/org/zzz (1920-1933)

Origines: naissance B√¢le /id/gpe/xxx (1896)

Relations familiales: √©pouse de Max M√ºller /id/person/www (1920-1945)
```

#### Organization

```python
def assemble_organization_text(org_id):
    parts = []
    parts.append(f"{prefLabel_fr}")
    if prefLabel_de:
        parts.append(f"({prefLabel_de})")
    
    if type:
        parts.append(f"\n\nType: {type}")
    
    if notice_institutionnelle:
        parts.append(f"\n\n{notice_institutionnelle}")
    
    # Hi√©rarchie
    parent_org = get_parent_organization(org_id)
    if parent_org:
        parts.append(f"\n\nFait partie de: {parent_org}")
    
    # Localisation
    if gpe_name:
        parts.append(f"\n\nLocalisation: {gpe_name}")
    
    # Sources
    sources = get_archive_sources(org_id)
    if sources:
        parts.append(f"\n\nSources: {', '.join(sources)}")
    
    return " ".join(parts)
```

#### GPE

```python
def assemble_gpe_text(gpe_id):
    parts = []
    parts.append(f"{prefLabel_fr}")
    if prefLabel_de:
        parts.append(f"({prefLabel_de})")
    
    if notice_geo:
        parts.append(f"\n\n{notice_geo}")
    
    if coordinates_lat and coordinates_lon:
        parts.append(f"\n\nCoordonn√©es: {lat}, {lon}")
    
    sources = get_archive_sources(gpe_id)
    if sources:
        parts.append(f"\n\nSources: {', '.join(sources)}")
    
    return " ".join(parts)
```

### √âtape 2 : G√©n√©ration Embeddings

M√™me processus que pour les documents (Gemini + rate limiting).

### √âtape 3 : Cr√©ation N≈ìuds

```cypher
CREATE (c:Chunk {
  id: "{entity_id}::entity_summary",
  text: "Elisabeth M√ºller. N√©e en 1896...",
  chunk_type: "entity_summary",
  embedding: [...],  // 768D Gemini
  embedding_model: "text-multilingual-embedding-002",
  embedding_provider: "vertex-ai",
  entity_id: "/id/person/{uuid}",
  entity_type: "Person",
  char_count: 820
})
```

### √âtape 4 : Relations

```cypher
// DESCRIBES_ENTITY
MATCH (c:Chunk {id: $chunk_id})
MATCH (e {id: $entity_id})
MERGE (c)-[:DESCRIBES_ENTITY {method: 'assembled'}]->(e)

// MENTIONS (extraction depuis IDs dans texte)
// Format d√©tect√© : "/id/person/xxx", "/id/org/yyy", "/id/gpe/zzz"
MATCH (c:Chunk {id: $chunk_id})
WHERE c.text CONTAINS "/id/org/96ad0f92"
MATCH (org:Organization {id: "/id/org/96ad0f92-75ac-4815-8c46-22a8f094b2b7"})
MERGE (c)-[:MENTIONS {source: 'text'}]->(org)
```

---

## üìä Index Vectoriel

### Cr√©ation

```cypher
CREATE VECTOR INDEX chunk_embeddings IF NOT EXISTS
FOR (c:Chunk) ON (c.embedding)
OPTIONS {
  indexConfig: {
    `vector.dimensions`: 768,
    `vector.similarity_function`: 'cosine'
  }
};
```

### Utilisation dans Claude Projects

**Configuration Similarity Search Tool** :

```yaml
Tool Type: Similarity Search
Name: Find Similar Biographical Passages
Description: Trouve des passages biographiques similaires dans les synth√®ses d'entit√©s

Embedding provider: Vertex AI
Embedding model: text-multilingual-embedding-002
Number of results: 5
```

### Requ√™te Cypher Enrichie

```cypher
// Recherche vectorielle hybride
CALL db.index.vector.queryNodes(
  'chunk_embeddings',
  20,                  // Top K
  $query_embedding     // Vector 768D depuis Vertex AI
)
YIELD node AS chunk, score
WHERE score >= 0.7

// Enrichir selon type de chunk
OPTIONAL MATCH (chunk {chunk_type: 'quote_centered'})-[:CHUNK_OF]->(doc:ArchiveDocument)
OPTIONAL MATCH (chunk)-[:DESCRIBES_EVENT]->(e:Event)
OPTIONAL MATCH (chunk)-[:DESCRIBES_ACTION]->(m:MicroAction)
OPTIONAL MATCH (chunk {chunk_type: 'entity_summary'})-[:DESCRIBES_ENTITY]->(entity)
OPTIONAL MATCH (chunk)-[:MENTIONS]->(mentioned)

RETURN 
  chunk.text AS text,
  chunk.chunk_type AS type,
  score,
  doc.title AS document_title,
  e.label AS event,
  m.label AS action,
  entity.prefLabel_fr AS entity_name,
  labels(entity)[0] AS entity_type,
  collect(DISTINCT mentioned.prefLabel_fr) AS mentions
ORDER BY score DESC
LIMIT 5;
```

---

## üöÄ Utilisation

### Configuration config.json

```json
{
  "vault_path": "/home/steeven/vault_obsidian",
  
  "neo4j": {
    "uri": "neo4j+s://xxxxx.databases.neo4j.io",
    "user": "neo4j",
    "password": "votre-mot-de-passe",
    "database": "neo4j"
  },
  
  "gcp": {
    "project_id": "gen-lang-client-0810997704",
    "location": "us-central1"
  },
  
  "embedding_model": "text-multilingual-embedding-002"
}
```

### Script Documents : `vectorize_chunks_gemini.py`

```bash
# Lancer vectorisation documents
python vectorize_chunks_gemini.py

# Options
python vectorize_chunks_gemini.py --no-fuzzy      # D√©sactiver fuzzy matching
python vectorize_chunks_gemini.py --no-adaptive   # D√©sactiver contexte adaptatif
```

**Sortie attendue** :

```
======================================================================
VECTORIZATION v3.0 - Gemini Embeddings Native
======================================================================

‚ú® Adaptive context ENABLED
   Min total: 400 chars
   Max total: 900 chars
‚ú® Fuzzy matching ENABLED (threshold: 85.0%)

üîç Checking prerequisites...
  ‚úÖ Found 376 Assertions

üîó Checking SUPPORTS direction...
  ‚úÖ Found 74 Documents with Assertions

üìñ Loading documents...
  ‚úÖ Loaded 75 documents

üìÑ 638731155645304456-001-16...
  üìå 2 assertions
  ‚úÖ Located 2/2 quotes (1 exact, 1 fuzzy)
  üì¶ 2 chunks
     Avg chunk size: 280 chars
  ‚úÖ Done

...

======================================================================
VECTORIZATION REPORT v3.0 - Gemini Embeddings
======================================================================

üìä STATISTICS
----------------------------------------------------------------------
Documents processed: 74
Assertions found: 376
Quotes located: 320
  - Exact matches: 70
  - Fuzzy matches: 250
Chunks created: 225
Embeddings generated (Gemini): 225
Chunk sizes: avg=650, min=420, max=890

üìà Quote coverage: 85.1%
   ‚úÖ Excellent coverage!

üîó RELATIONS
----------------------------------------------------------------------
DESCRIBES_EVENT: 75
DESCRIBES_ACTION: 150
MENTIONS: 1072

======================================================================
‚úÖ SUCCESS: 225 chunks vectorized with Gemini!
   üöÄ Compatible with Claude Projects Similarity Search
======================================================================
```

### Script Entit√©s : `vectorize_entities_gemini.py`

```bash
python vectorize_entities_gemini.py
```

**Sortie attendue** :

```
======================================================================
ENTITY VECTORIZATION v2.0 - Gemini Embeddings Native
======================================================================

üìã Processing Person entities...
  Found 48 Person entities
    Processed 10 entities...
    Processed 20 entities...
    ...
  ‚úÖ Completed Person

üìã Processing Organization entities...
  Found 29 Organization entities
    Processed 10 entities...
    ...
  ‚úÖ Completed Organization

üìã Processing GPE entities...
  Found 23 GPE entities
  ‚úÖ Completed GPE

======================================================================
ENTITY VECTORIZATION REPORT v2.0 - Gemini Embeddings
======================================================================

üìä STATISTICS
----------------------------------------------------------------------
Entities processed: 91
Chunks created: 91
Embeddings generated (Gemini): 91
Narrative sections found: 248
Occupations found: 17
Origins found: 6
Family relations found: 9
  ‚Üí Average 2.7 narrative sections per entity
  ‚Üí Average 0.2 occupations per entity

üîó RELATIONS
----------------------------------------------------------------------
DESCRIBES_ENTITY: 91
MENTIONS: 30

‚ö†Ô∏è ERRORS (9)
----------------------------------------------------------------------
  ‚Ä¢ Embedding failed for /id/person/1c6bf450... (texte trop court)
  ‚Ä¢ Embedding failed for /id/org/96ad0f92... (quota temporaire)
  ...

======================================================================
‚úÖ SUCCESS: 91 entity chunks created!
   Plus 30 MENTIONS relations
   üöÄ Using Gemini embeddings for Claude agent
======================================================================
```

### Retry Entit√©s √âchou√©es (Optionnel)

```bash
# Script pour retry les 9 entit√©s √©chou√©es
python retry_failed_entities.py
```

### Pipeline Complet Claude Projects

**Dans Claude Projects, le tool Similarity Search utilise** :

1. **Embedding automatique** : Vertex AI g√©n√®re l'embedding de la query
2. **Recherche Neo4j** : Via l'index `chunk_embeddings`
3. **Top K r√©sultats** : 5 chunks les plus similaires
4. **Context enrichi** : Claude utilise les chunks + relations du graphe

**Exemple d'utilisation** :

```
User: "O√π √©tait d√©tenue Elisabeth M√ºller en octobre 1942 ?"

Claude ‚Üí Similarity Search Tool:
  - Embedding: text-multilingual-embedding-002
  - Query: "O√π √©tait d√©tenue Elisabeth M√ºller en octobre 1942"
  - Results: 5 chunks (3 documents + 2 entit√©s)

Claude re√ßoit:
  1. Chunk doc: "...Elisabeth M√ºller est intern√©e au camp de Gurs..."
  2. Chunk doc: "...conditions de d√©tention difficiles en octobre..."
  3. Chunk entity: "Elisabeth M√ºller. R√©fugi√©e juive..."
  4. ...

Claude ‚Üí R√©ponse enrichie:
  "D'apr√®s les archives, Elisabeth M√ºller √©tait d√©tenue au camp 
   de Gurs en octobre 1942. Les documents mentionnent..."
```

---

## ‚úÖ Validation

### Tests Ex√©cut√©s sur Corpus R√©el

#### Test 1 : Chunks cr√©√©s par type

```cypher
MATCH (c:Chunk)
RETURN c.chunk_type, count(*) AS count
ORDER BY count DESC;
```

**Attendu** :

```
quote_centered    225
entity_summary     91
```

#### Test 2 : Embeddings Gemini valides

```cypher
MATCH (c:Chunk)
WHERE c.embedding IS NOT NULL 
  AND size(c.embedding) = 768
  AND c.embedding_provider = 'vertex-ai'
  AND c.embedding_model = 'text-multilingual-embedding-002'
RETURN count(*) AS chunks_with_gemini_embeddings;
```

**Attendu** : 316

#### Test 3 : Relations DESCRIBES

```cypher
CALL {
  MATCH ()-[r:DESCRIBES_EVENT]->()
  RETURN 'DESCRIBES_EVENT' AS rel, count(r) AS count
  UNION
  MATCH ()-[r:DESCRIBES_ACTION]->()
  RETURN 'DESCRIBES_ACTION' AS rel, count(r) AS count
  UNION
  MATCH ()-[r:DESCRIBES_ENTITY]->()
  RETURN 'DESCRIBES_ENTITY' AS rel, count(r) AS count
}
RETURN rel, count;
```

**Attendu** :

```
DESCRIBES_EVENT    75
DESCRIBES_ACTION   150
DESCRIBES_ENTITY   91
```

#### Test 4 : MENTIONS par type

```cypher
MATCH (c:Chunk)-[r:MENTIONS]->()
RETURN c.chunk_type, count(r) AS mentions
ORDER BY mentions DESC;
```

**Attendu** :

```
quote_centered    1072
entity_summary      30
```

#### Test 5 : Index vectoriel

```cypher
SHOW INDEXES 
YIELD name, type, entityType, labelsOrTypes, properties
WHERE name = 'chunk_embeddings'
RETURN name, type, entityType, labelsOrTypes, properties;
```

**Attendu** :

```
name: chunk_embeddings
type: VECTOR
entityType: NODE
labelsOrTypes: ["Chunk"]
properties: ["embedding"]
```

#### Test 6 : V√©rifier compatibilit√© Vertex AI

```cypher
MATCH (c:Chunk)
WHERE c.embedding_provider = 'vertex-ai'
  AND c.embedding_model = 'text-multilingual-embedding-002'
RETURN c.chunk_type, count(*) as count;
```

**Attendu** :

```
quote_centered    225
entity_summary     91
```

#### Test 7 : Chunks entit√©s par type

```cypher
MATCH (c:Chunk {chunk_type: 'entity_summary'})-[:DESCRIBES_ENTITY]->(e)
RETURN labels(e)[0] AS entity_type, count(*) AS count
ORDER BY count DESC;
```

**Attendu** :

```
Person          45 (48 - 3 √©checs)
Organization    27 (29 - 2 √©checs)
GPE             19 (23 - 4 √©checs)
Total           91
```

#### Test 8 : Recherche vectorielle test

```cypher
// Test avec embedding dummy (remplacer par vrai embedding Gemini)
WITH [0.1, 0.2, ...] AS test_embedding  // 768 dimensions

CALL db.index.vector.queryNodes(
  'chunk_embeddings',
  5,
  test_embedding
)
YIELD node AS chunk, score

RETURN chunk.text, chunk.chunk_type, score
ORDER BY score DESC;
```

---

## üîß Troubleshooting

### Erreur 429 : Quota Exceeded

**Sympt√¥me** :

```
‚ö†Ô∏è  Embedding error: 429 Quota exceeded for aiplatform.googleapis.com/
    online_prediction_requests_per_base_model
```

**Causes** :

- Trop de requ√™tes par minute (quota d√©faut : 60/min)
- Compte GCP nouveau avec quotas par d√©faut

**Solutions** :

1. **Le rate limiting est d√©j√† int√©gr√©** dans v3.0 :
    
    - Attente de 1s entre chaque requ√™te
    - Exponential backoff : 2s, 4s, 8s, 16s, 32s
    - 5 tentatives automatiques
2. **Augmenter quota GCP** :
    
    ```
    1. https://console.cloud.google.com/iam-admin/quotas
    2. Filtrer : "aiplatform.googleapis.com/online_prediction_requests"
    3. S√©lectionner quota
    4. EDIT QUOTAS ‚Üí Demander 600/min
    5. Justification : "Historical research project"
    6. Attendre approbation (24-48h)
    ```
    
3. **Ralentir davantage** (si n√©cessaire) :
    
    ```python
    # Dans vectorize_chunks_gemini.py, ligne ~550
    base_delay = 2.0  # Au lieu de 1.0
    ```
    

### Erreur : Invalid credentials

**Sympt√¥me** :

```
google.auth.exceptions.DefaultCredentialsError: Could not automatically 
determine credentials
```

**Solution** :

```bash
# V√©rifier variable d'environnement
echo $GOOGLE_APPLICATION_CREDENTIALS

# Si vide, configurer
export GOOGLE_APPLICATION_CREDENTIALS="$HOME/.gcp/vertex-ai-key.json"

# V√©rifier fichier existe
ls -la $GOOGLE_APPLICATION_CREDENTIALS
```

### Erreur : Project ID non trouv√©

**Sympt√¥me** :

```
KeyError: 'gcp'
```

**Solution** : Ajouter section `gcp` dans `config.json` :

```json
{
  "gcp": {
    "project_id": "gen-lang-client-0810997704",
    "location": "us-central1"
  }
}
```

### Embeddings √©chouent pour certaines entit√©s

**Sympt√¥me** :

```
‚ö†Ô∏è ERRORS (9)
  ‚Ä¢ Embedding failed for /id/person/xxx
```

**Causes possibles** :

1. Texte trop court (< 20 caract√®res)
2. Quota 429 pour ces requ√™tes sp√©cifiques
3. Entit√© sans contenu (pas de notice_bio/inst/geo)

**Diagnostic** :

```cypher
MATCH (e {id: "/id/person/1c6bf450-3c41-4755-be82-d77f9744f6e1"})
RETURN e.prefLabel_fr, 
       e.notice_bio,
       length(coalesce(e.notice_bio, '')) AS bio_length;
```

**Solution** :

```bash
# Retry avec script d√©di√©
python retry_failed_entities.py
```

### Fuzzy matching trop lent

**Sympt√¥me** : Vectorisation tr√®s lente (> 30 minutes)

**Solution** : D√©sactiver fuzzy matching :

```bash
python vectorize_chunks_gemini.py --no-fuzzy
```

**Trade-off** :

- Plus rapide
- Moins de quotes localis√©es (seulement exact match)

### Index vectoriel non cr√©√©

**Sympt√¥me** :

```
SHOW INDEXES ‚Üí chunk_embeddings non pr√©sent
```

**Solution** :

```cypher
// Supprimer si existe
DROP INDEX chunk_embeddings IF EXISTS;

// Recr√©er
CREATE VECTOR INDEX chunk_embeddings
FOR (c:Chunk) ON (c.embedding)
OPTIONS {
  indexConfig: {
    `vector.dimensions`: 768,
    `vector.similarity_function`: 'cosine'
  }
};
```

### Claude Projects ne trouve pas les chunks

**Sympt√¥me** : Le tool Similarity Search retourne 0 r√©sultats

**V√©rifications** :

```cypher
// 1. Chunks existent ?
MATCH (c:Chunk) RETURN count(c);

// 2. Embeddings Vertex AI ?
MATCH (c:Chunk) 
WHERE c.embedding_provider = 'vertex-ai'
RETURN count(c);

// 3. Index actif ?
SHOW INDEXES WHERE name = 'chunk_embeddings';

// 4. Test recherche manuelle
CALL db.index.vector.queryNodes(
  'chunk_embeddings', 
  5, 
  [0.1, 0.2, ...]  // 768 dimensions
) YIELD node, score
RETURN node.text, score;
```

---

## üìù Notes Techniques

### Rate Limiting Automatique

Le script v3.0 impl√©mente :

- **Base delay** : 1s entre chaque requ√™te
- **Exponential backoff** : 2^attempt secondes en cas d'erreur 429
- **Max retries** : 5 tentatives

**Temps estim√©** :

- 225 chunks documents √ó 1.5s = ~6 minutes
- 91 chunks entit√©s √ó 1.5s = ~2.5 minutes
- **Total** : ~10 minutes (vs 2 minutes sans rate limiting)

### Co√ªts Vertex AI

Pour un projet typique (316 chunks, 650 chars moyenne) :

```
316 chunks √ó 650 chars = 205,400 caract√®res
205,400 / 1000 = 205.4 unit√©s
205.4 √ó $0.00001 = $0.002054

Co√ªt total : ~0.002$ (moins d'un centime)
```

**Avec cr√©dits gratuits (300$)** : ~150 millions de chunks possibles !

### Entit√©s sans embeddings : Normal

9/100 entit√©s ont √©chou√©, c'est **acceptable** car :

1. Souvent = entit√©s avec tr√®s peu de contenu
2. 91% de couverture reste excellent
3. Recherche s√©mantique compensera les manquantes
4. Retry disponible si critique

### MENTIONS depuis structures r√©ifi√©es

Les MENTIONS sont extraites depuis les **IDs dans le texte** :

```
"Occupations: Employ√©e √† Organisation Suisse /id/org/96ad0f92..."
                                               ^^^^^^^^^^^^
                                               D√©tect√© et li√© !
```

C'est pourquoi les chunks d'entit√©s ont 30 MENTIONS (depuis occupations, origins, relations familiales).

### Multilingue FR/DE

`text-multilingual-embedding-002` g√®re nativement :

- Fran√ßais
- Allemand
- 100+ autres langues

Parfait pour corpus diplomatique CH multilingue !

---
